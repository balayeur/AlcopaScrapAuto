import requests
import sqlite3
import datetime
import locale
from bs4 import BeautifulSoup

URL = "https://www.alcopa-auction.fr/"
DB_FILE = "auctions.db"
HTML_FILE = "mnt/data/AlcopaAuctionVenteEnCours02.html"

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫—É—é –ª–æ–∫–∞–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞—Ç–∞–º–∏
locale.setlocale(locale.LC_TIME, "fr_FR.UTF-8")
# locale.setlocale(locale.LC_TIME, "C.UTF-8")


# –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –º–µ—Å—è—Ü–µ–≤
MONTHS_FR = {
    "janv.": "01", "f√©vr.": "02", "mars": "03", "avr.": "04",
    "mai": "05", "juin": "06", "juil.": "07", "ao√ªt": "08",
    "sept.": "09", "oct.": "10", "nov.": "11", "d√©c.": "12"
}

def load_html(file_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML –∏ —Å–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç BeautifulSoup."""
    with open(file_path, "r", encoding="utf-8") as file:
        return BeautifulSoup(file, "html.parser")

def fetch_html(url):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –≤–µ–±-—Å–∞–π—Ç–∞."""
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    response.raise_for_status()  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—à–∏–±–∫–∏ –∑–∞–ø—Ä–æ—Å–∞
    return BeautifulSoup(response.text, "html.parser")

def create_database():
    """–°–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç."""
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS auctions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            category TEXT,
            description TEXT,
            location TEXT,
            lots TEXT,
            date TEXT,
            link TEXT UNIQUE,
            linkLive TEXT
        )
    """)
    conn.commit()
    conn.close()

def insert_into_database(category, description, location, lots, date, link, linkLive):
    """–í—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –±–∞–∑—É, –µ—Å–ª–∏ –∑–∞–ø–∏—Å–∏ –µ—â–µ –Ω–µ—Ç."""
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()
    try:
        cursor.execute("""
            INSERT INTO auctions (category, description, location, lots, date, link, linkLive)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (category, description, location, lots, date, link, linkLive))
        conn.commit()
    except sqlite3.IntegrityError:
        print(f"‚ö† –ó–∞–ø–∏—Å—å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {location} - {description} - {date} - {link} - {linkLive}")
    conn.close()

def convert_timestamp(ts):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç UNIX timestamp –≤ –¥–∞—Ç—É."""
    return datetime.datetime.fromtimestamp(int(ts)).strftime("%Y-%m-%d %H:%M:%S")

def convert_french_date(date_str):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞—Ç—É 'lun. 10 f√©vr.' –≤ 'YYYY-MM-DD'."""
    parts = date_str.split()
    if len(parts) != 3:
        return "Non pr√©cis√©"
    day = parts[1]
    month = MONTHS_FR.get(parts[2], "00")
    year = datetime.datetime.now().year
    return f"{year}-{month}-{day.zfill(2)}"

def parse_sales(soup):
    """–ü–∞—Ä—Å–∏—Ç –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—É."""
    sales = {"Vente en cours": [], "Vente en Salle": [], "Vente Web": [], "Vente de mat√©riel en salle": []}
    for row in soup.find_all("div", class_="row"):
        cols = row.find_all("div", class_="col-md-12", recursive=False)
        # cols = row.find_all("div", class_="d-table w-100 mb-2 rounded border no-decoration bg-graylight")
        # if len(cols) < 2:
        #     continue
        if not cols:
            continue
        for col in cols:
            h4 = col.find("h4")
            if not h4:
                continue

            category_name = h4.get_text(strip=True)
            if "Vente en Salle" in category_name:
                sale_category = "Vente en Salle"
            elif "Vente en cours" in category_name:
                sale_category = "Vente en cours"            
            elif "Vente Web" in category_name: # or "Vente web Madrid" in category_name:
                sale_category = "Vente Web"
            elif "Vente de mat√©riel en salle" in category_name:
                sale_category = "Vente de mat√©riel en salle"
            else:
                continue

            divs = row.find_all("div", class_="d-table w-100 mb-2 rounded border no-decoration bg-graylight")
            for div in divs: #[1:]:
                # div = div.find("div", class_="d-table w-100 mb-2 rounded border no-decoration bg-graylight")
                # if not div:
                #     continue
                try:
                    location = div.find("span", class_="font-weight-bold").text.strip()
                    lots = div.find("span", class_="text-graynorm").text.strip()
                    descr = div.find("div", class_="text-graynorm mt-1 pt-1 border-top lh-20").text.strip()
                    date = "Non pr√©cis√©"
                    link = URL + div.find("a", class_="sale-access-href")["href"].lstrip("/")
                    linkLiveSale = "Non pr√©cis√©"

                    if sale_category == "Vente Web":
                        ts_span = div.find("span", class_="js-countdown-time")
                        if ts_span and ts_span.has_attr("data-ts"):
                            date = convert_timestamp(ts_span["data-ts"])
                    elif sale_category == "Vente en cours":
                        date = "En cours"
                    elif sale_category == "Vente en Salle":
                        date_tag = div.find("div", class_="float-right")
                        if date_tag:
                            date = convert_french_date(date_tag.text.strip())
                        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Å—ã–ª–∫—É live
                        parts = link.split("/")
                        city = parts[4]  # –ù–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "beauvais")
                        sale_id = parts[5]  # –ù–æ–º–µ—Ä (–Ω–∞–ø—Ä–∏–º–µ—Ä, "8058")                    
                        linkLiveSale = f"https://live-{city}.alcopa-auction.fr/{sale_id}"

                    sale_data = (location, descr, lots, date, link, linkLiveSale)
                    if sale_data not in sales[sale_category]:
                        sales[sale_category].append(sale_data)
                except AttributeError:
                    continue
    return sales

def main():
    create_database()
    # soup = fetch_html(URL)
    soup = load_html(HTML_FILE)
    sales_data = parse_sales(soup)
    for category, items in sales_data.items():
        print(f"\nüîπ {category}:")
        for sale in items:
            print(f"üìç {sale[0]} - {sale[1]} - {sale[2]} - {sale[3]} - üîó {sale[4]} - üîó {sale[5]}")
            insert_into_database(category, sale[0], sale[1], sale[2], sale[3], sale[4], sale[5])

if __name__ == "__main__":
    main()
